https://www.udemy.com/course/apache-kafka-for-beginners/learn/lecture/17425986#overview

Kafka Core Concepts
	1)Producer
	2)Consumer
	3)Broker
	4)Cluster
	5)Topic
		A unique name for a given data sets
	6)Partitions
		Partition is a smallest unit sitting on a single machine.
	7)Partition Offset
	8)Consumer Groups
		no of partitions >= consumers in consumer group
			so that double reading of record not happens.


Kafka Connect Core Concepts
	Kafka Connect 
		Connect is a component of Kafka for connecting and moving data between Kafka and external systems.
		We have two types of Kafka Connectors. 
			Source Connector 
			Sink connector
		And together, they support a bunch of systems and offer you an out of the box data integration capability 
			without writing a single line of code.

	How Kafka Connect works?
		Kafka Connect Source connector.
			It is a system which you can place in between your data source and data Cluster.
			Then all you do is to configure it to consume data from this source system and send it to the Kafka Cluster.
			You do not need to write a single line of code.
			Everything is already done and made available to you.
			You'll just configure and run, and the Kafka Connect will do the job for you.
			The Source Connector will internally use the Kafka producer API.
		Kafka Connect Sink connector
			The Sink Connector will internally use the Kafka consumer API.
		
		Kafka Connect framework
			Source Connector 
				SourceConnector
				SourceTask
			Sink connector
				SinkConnector
				SinkTask
			The Kafka connect framework takes care of all the heavy lifting, scalability, fault tolerance, error handling, 
			and bunch of other things. 
			As a connector developer, all you need to do is to implement two Java classes.
			The first one is SourceConnector or SinkConnector class. 
			And the second one is the SourceTask or the SinkTask.

	Kafka Connect Scalability
		The Kafka Connect itself is a Cluster..
		Each individual unit in the Connect Cluster is called a Connect Worker.
		Connect Worker can have both Source & Sink task simultaneously.

	Kafka Connect Transformations
		Connect was designed to perform a plane copy our data movement between third party systems and Kafka.
			In both the cases (Source or Sink), one side must be a Kafka Cluster.
			However, Kafka connect also allowed some fundamental Single Message Transformations (SMTs). 
			SMTs
				Add a new field in your record using static data or metadata
				Filter or rename fields
				Mask Some fields with a Null value
				Change the record key
				Route the record to a different Kafka topic
			You can chain multiple SMTs and play with it to restructure your records and route them to a different topic.
			However, these SMTs are not good enough to perform some real life data validations and transformations.

	Kafka Connect Architecture
		1)Workers 
			Workers with same worker-group_id
			Workers are fault tolerant & self-managed
			Workers provides
				1)High availability
				2)Scallability
				3)Load Balancing
				4)Reliability 
		2)Connector
			Download & Install connectoron Workers
			Configure connector
				DB connection details
				Source table list
				Polling frequency
				Max Parallelism
			Start the connector	
				one of the workers will start your Connector process.
				1)Using command line tool
				2)Using rest api
			DB conneciton details =XXXX
			Source table list = T1,T2,T3,T4
			Polling frequency =5 min
			Max Parallelism =5
			Connector will not copy the data from source to Kafka.
				Connector is responsible only for defining & creating task list.
				connector process is mainly responsible for two things.
				The first thing is to determine the degree of parallelism.
			TaskName=Task1 TaskCOnfigs=XXXX
			TaskName=Task2 TaskCOnfigs=XXXX
			TaskName=Task3 TaskCOnfigs=XXXX
			TaskName=Task4 TaskCOnfigs=XXXX
			Finally, the list of tasks will be given to these workers, and they will start the task. 
			So your task is distributed across the workers for balancing the Cluster load.
			Now the task is responsible for connecting to the source system, polling the data at a regular interval,
			collecting the records, and handing over it to the worker. Yes, that's correct.
			They do not send the record to the Kafka Cluster.	
			That task is only responsible for interacting with the external system.
			This source task will handover the data to the worker, and the worker is responsible for sending it to the Kafka.
		3)Task
				1)Task split
				2)Task list & configuration
		Reading and writing data to a Kafka Cluster is a standard activity.
			So it is taken care of by the framework.
		We have two things that are changing for different source and target systems.
			How to split the input for parallel processing.
				This is taken care of by the Connector class. 
			How to interact with the external system.
				This is also taken care of by the Task class. 
			And these are the things that are connector developer needs to take care of. 
			Most of the other stuff like interacting with Kafka, handling configurations, errors, monitoring connectors, 
				and tasks, scaling up and down, and handling failures are standard things and are taken care of by the Kafka Connect Framework. 
	
	
	
Kafka Streams Core Concepts
	What is real-time stream processing?
	WHat is Kafka?
	Kafka Stream Architecture
	
	What is real-time stream processing?
		Data streams
			unbounded-no definite starting & ending 	
			Often infinite & ever growing
			Sequence of data in small packets(KB)
			Example:
				Sensors
				Log entries
				Click Streams
				Transactions
				Data feeds
	WHat is Kafka Streams?
		A java/scala library
		Input data must be in Kafka topic
		We can embeded Kafka streams in our microservices
		Deploy anywhere(no cluster needed)
		Out of the box parallel processing, scalability, & fault tolerance
	What Kafka Stream offers?
		1.Working with streams/tables & interoperating with them
		2.Grouping & continiously updating aggregates
		3.Join streams, tables & a combination of both
		4.Create & manage fault-tolerant, efficientlocal state stores
		5.Flexible windowing capability
		6.Flexible time schemantics- Even time, Processing time, Latecomers, High Watermark, Eaxctly -once processing etc.
		7. Interactive query-Serving other microservices
		8.Unit testing tools
		9.Easy to use DLS & eextensibility to create custom processors
		10.Inherent fault tolerance & dynamic scalability
		11.Deploy in containers & manage using Kubernates
	Kafka Stream Architecture
		Kafka streams is all about continuously reading a stream of data from one or more Kafka topics.
			And then, you develop your application logic to process those streams in real-time and take necessary actions.
		We deploy your application on a single machine. 
			Now the Kafka streams will internally create three logical tasks because the maximum number of partitions across the input topics T1 and T2 are three.
			
			
KSQL CORE CONCEPTS
	It is a SQL interface to Kafka Streams.
	So in very short, most of the things which you can do using Kafka Streams are available to you in KSQL.
	It means you can create scalable and fault-tolerant stream processing workloads 
		without the need to write code in a programming language such as Java or Scala. 
	KSQL runs in 2 modes:
		Interactive mode
			 Interactive mode is using
			 	 a command-line interface (CLI) or 
				 a web-based UI to submit KSQL and get an immediate response.
		Headless Mode		
			The Headless mode is a non-interactive mode that allows you to submit your KSQL files, which are executed by the KSQL server.
		The headless mode is ideal for the production environment, whereas the CLI mode is ideal for the development environment.
		The KSQL server can be deployed in one of the available modes. 
mode and Headless Mode.
	KSQL Architecture
		1)KSQL Server
			i)KSQL engine
			ii)REST interface
		2)KSQL Client(CLI/UI)
	You can also deploy multiple KSQL servers to form a scalable KSQL cluster.

	The KSQL engine is the core component which is responsible for KSQL statement and queries. 
	Under the hood, the engine is going to parse your KSQL statements, build corresponding Kafka streams topology,
	and run them as streams tasks.
	And these are streams tasks are executed on the available KSQL servers in the cluster.
	You can dynamically add more servers in the cluster to scale out the resources, 
	and fault Tolerance is an inherent feature of the Kafka streams.
	I hope I don't need to mention that the KSQL cluster is separate from your Kafka cluster, 
		and your KSQL Server will internally communicate to the Kafka cluster for reading inputs and writing outputs.
	The REST interface is to power the KSQL clients. So, the KSQL CLient will send the commands to the REST Interface, 
		which will internally communicate with the KSQL Engine to execute your KSQL Commands.
	Now the next question - What can you do with KSQL.
	In a nutshell, KSQL allows you to use your Kafka topic as a table and fire SQL like queries over those topics. 
	With that power in your hand, You can imagine the kind of things that you can do.
	KSQL allows us to 
		1)Grouping & aggregating on our Kafka topics
		2)Grouping & aggregating over a time window 
		3)Apply filters
		4)Join two topics
		5)Sink the result of query into another topic
		So the KSQL for Kafka is one big step forward for Kafka to become a real time data warehouse.
		Those days are not too far when you might see JDBC/ODBC connectors being available for KSQL
			and visualization tools like Tableau and QlikView to start connecting with the KSQL.
			I'm not predicting anything, but that's the next logical step for Kafka and KSQL.

When to use what?
	1)Kafka Broker
	2)Kafka CLient API
	3)Kafka Connect
	4)Kafka Streams
	5)KSQL	
			
Kafka Solution Patterns
	1)Data integration pattern()
		1)Decoupling of producers from consumers
		2)Reliability & fault tolerance
		3)Horizontal Scalability
		4)Time sensitivity & performance
		5)Extensibility
		It will use Kafka Broker, Kafka Client & Kafka Connect
	2)Microservice architecture for stream processing
		It will use Kafka Broker & Kafka Streams
	3)Real-time Streaming in Data Warehouse & Data lakes
		It will use Kafka Broker & KSQL
		
		
		
Data Ingestion/Integration 
	Kafka Broker & internals
	Kafka Connect & commonly used connecotrs
Microservices Architecture
	Kafka Borkers & internals
	Kafka client APIs
	Kafka Streams
Data Engineering
	Kafka Borkers & internals
	Interacting with Kafka using Spark Structured streaming
	

			